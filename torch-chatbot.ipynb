{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cornell movie-dialogs corpus\\movie_lines.txt\n",
      "cornell movie-dialogs corpus\\movie_conversations.txt\n"
     ]
    }
   ],
   "source": [
    "lines_filepath = os.path.join(\"cornell movie-dialogs corpus\", \"movie_lines.txt\")\n",
    "conv_filepath = os.path.join(\"cornell movie-dialogs corpus\", \"movie_conversations.txt\")\n",
    "\n",
    "print(lines_filepath)\n",
    "print(conv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
      "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
      "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
      "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
      "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n"
     ]
    }
   ],
   "source": [
    "# Visualize some lines in file\n",
    "with open(lines_filepath, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines[:8]:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into a dictionary of fields (lineID, characterID, movieID, character, text)\n",
    "line_fields = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "lines = {}\n",
    "\n",
    "with open(lines_filepath, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' +++$+++ ')\n",
    "        lineObj = {}\n",
    "        for i, field in enumerate(line_fields):\n",
    "            lineObj[field] = values[i]\n",
    "        lines[lineObj['lineID']] = lineObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_fields = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "conversations = []\n",
    "\n",
    "with open(conv_filepath, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' +++$+++ ')\n",
    "        convObj = {}\n",
    "        for i, field in enumerate(conv_fields):\n",
    "            convObj[field] = values[i]\n",
    "        lineIDs = eval(convObj['utteranceIDs'])\n",
    "        \n",
    "        convObj[\"lines\"] = []\n",
    "        for lineID in lineIDs:\n",
    "            convObj[\"lines\"].append(lines[lineID])\n",
    "        conversations.append(convObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pairs of sentences from conversations\n",
    "qa_pairs = []\n",
    "for conversation in conversations:\n",
    "    for i in range(len(conversation[\"lines\"]) - 1):\n",
    "        inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "        targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "        if inputLine and targetLine:\n",
    "            qa_pairs.append([inputLine, targetLine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delimiter:  \t\n",
      "Writting newly formatted file...\n",
      "Done writting to file\n"
     ]
    }
   ],
   "source": [
    "# Define path to new file\n",
    "datafile = os.path.join(\"cornell movie-dialogs corpus\", \"formatted_movie_lines.txt\")\n",
    "delimiter = \"\\t\"\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "print(\"Delimiter: \", delimiter)\n",
    "\n",
    "print(\"Writting newly formatted file...\")\n",
    "with open(datafile, \"w\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=delimiter)\n",
    "    for pair in qa_pairs:\n",
    "        writer.writerow(pair)\n",
    "print(\"Done writting to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\r\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\r\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "# Visualize some lines\n",
    "datafile = os.path.join(\"cornell movie-dialogs corpus\", \"formatted_movie_lines.txt\")\n",
    "with open(datafile, 'rb') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0 # padding short sentence\n",
    "SOS_token = 1 # starting of sentence\n",
    "EOS_token = 2 # ending of sentence\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    # remove word below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        print(\"keep_word {}/{} = {:.4f}\".format(len(keep_words), len(self.word2index), len(keep_words)/len(self.word2index)))\n",
    "        # Reintialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn unicode string to ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != \"Mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Montreal,Francois...'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test unicodeToAscii\n",
    "unicodeToAscii(\"Montréal,François...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, trim whitespace, lines... etc and remove non-leter character\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaa aa !s s dd ?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test normalizeString\n",
    "normalizeString(\"aaa123aa!s's   dd?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing file...Please wait\n",
      "Done reading!\n"
     ]
    }
   ],
   "source": [
    "datafile = os.path.join(\"cornell movie-dialogs corpus\", \"formatted_movie_lines.txt\")\n",
    "\n",
    "print(\"Reading and processing file...Please wait\")\n",
    "lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "pairs = [[normalizeString(s) for s in pair.split(\"\\t\")] for pair in lines]\n",
    "print(\"Done reading!\")\n",
    "\n",
    "vocab = Vocabulary(\"cornell movie-dialogs corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "def filterPair(p):\n",
    "    return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 221282 pairs/conversations in the dataset\n",
      "After filtering, there are 64271 pairs/conversations in the dataset\n"
     ]
    }
   ],
   "source": [
    "pairs = [pair for pair in pairs if len(pair) > 1]\n",
    "print(\"There are {} pairs/conversations in the dataset\".format(len(pairs)))\n",
    "pairs = filterPairs(pairs)\n",
    "print(\"After filtering, there are {} pairs/conversations in the dataset\".format(len(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:  18008\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs:\n",
    "    vocab.addSentence(pair[0])\n",
    "    vocab.addSentence(pair[1])\n",
    "print(\"Counted words: \", vocab.num_words)\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 3 # threshold\n",
    "\n",
    "def trimRareWords(vocab, pairs, MIN_COUNT):\n",
    "    vocab.trim(MIN_COUNT)\n",
    "    \n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        \n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in vocab.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in vocab.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "        \n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs)/len(pairs)))\n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_word 7823/18005 = 0.4345\n",
      "Trimmed from 64271 pairs to 53165, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "pairs = trimRareWords(vocab, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.word2index[word] for word in sentence.split(\" \")] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have my word . as a gentleman\n",
      "[7, 8, 9, 10, 4, 11, 12, 13, 2]\n"
     ]
    }
   ],
   "source": [
    "# Test indexesFromSentence\n",
    "print(pairs[1][0])\n",
    "print(indexesFromSentence(vocab, pairs[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there .', 'you have my word . as a gentleman', 'hi .', 'have fun tonight ?', 'well no . . .', 'then that s all you had to say .', 'but', 'do you listen to this crap ?', 'what good stuff ?', 'wow', 'she okay ?', 'they do to !', 'did you change your hair ?', 'no .', 'who ?']\n",
      "[[3, 4, 2], [7, 8, 9, 10, 4, 11, 12, 13, 2], [16, 4, 2], [8, 31, 22, 6, 2], [33, 34, 4, 4, 4, 2], [35, 36, 37, 38, 7, 39, 40, 41, 4, 2], [42, 2], [47, 7, 48, 40, 45, 49, 6, 2], [50, 51, 52, 6, 2], [58, 2], [61, 62, 6, 2], [65, 47, 40, 66, 2], [68, 7, 69, 70, 71, 6, 2], [34, 4, 2], [77, 6, 2]]\n"
     ]
    }
   ],
   "source": [
    "inp = []\n",
    "out = []\n",
    "i = 0\n",
    "for pair in pairs[:15]:\n",
    "    inp.append(pair[0])\n",
    "    out.append(pair[1])\n",
    "indexes = [indexesFromSentence(vocab, sentence) for sentence in inp]\n",
    "print(inp)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(l, fillvalue=0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng = [len(ind) for ind in indexes]\n",
    "max(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 3, 5, 6, 10, 2, 8, 5, 2, 4, 5, 7, 3, 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 7, 16, 8, 33, 35, 42, 47, 50, 58, 61, 65, 68, 34, 77),\n",
       " (4, 8, 4, 31, 34, 36, 2, 7, 51, 2, 62, 47, 7, 4, 6),\n",
       " (2, 9, 2, 22, 4, 37, 0, 48, 52, 0, 6, 40, 69, 2, 2),\n",
       " (0, 10, 0, 6, 4, 38, 0, 40, 6, 0, 2, 66, 70, 0, 0),\n",
       " (0, 4, 0, 2, 4, 7, 0, 45, 2, 0, 0, 2, 71, 0, 0),\n",
       " (0, 11, 0, 0, 2, 39, 0, 49, 0, 0, 0, 0, 6, 0, 0),\n",
       " (0, 12, 0, 0, 0, 40, 0, 6, 0, 0, 0, 0, 2, 0, 0),\n",
       " (0, 13, 0, 0, 0, 41, 0, 2, 0, 0, 0, 0, 0, 0, 0),\n",
       " (0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " (0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = zeroPadding(indexes)\n",
    "print(len(test_result))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryMatrix(l, value=0):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       " [0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0],\n",
       " [0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0],\n",
       " [0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_result = binaryMatrix(test_result)\n",
    "binary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputVar(l, vocab):\n",
    "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputVar(l, vocab):\n",
    "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2TrainData(vocab, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, vocab)\n",
    "    out, mask, max_target_len = outputVar(output_batch, vocab)\n",
    "    return inp, lengths, out, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable\n",
      "tensor([[ 654,    7, 6763,   25,   50],\n",
      "        [  25,  250, 6762,  200,   37],\n",
      "        [ 247,   44, 1766,  483,  101],\n",
      "        [ 117,   96,   40,  572,  206],\n",
      "        [  24,  180,  359,   25,   75],\n",
      "        [   7,   56,    7,  102,    6],\n",
      "        [ 102,  960,    4,  273,    2],\n",
      "        [ 467,   19,    2,    2,    0],\n",
      "        [   4,    4,    0,    0,    0],\n",
      "        [   2,    2,    0,    0,    0]])\n",
      "lengths\n",
      "tensor([10, 10,  8,  8,  7])\n",
      "output_varialbe\n",
      "tensor([[ 192,   25,   77,  112,   59],\n",
      "        [   4,  425,    6,  197,   83],\n",
      "        [ 534,    7,    2,  117,   60],\n",
      "        [  12,    8,    0,    4,   66],\n",
      "        [1230,    4,    0,    2,    2],\n",
      "        [1382,    4,    0,    0,    0],\n",
      "        [ 154,    4,    0,    0,    0],\n",
      "        [ 164,    2,    0,    0,    0],\n",
      "        [   4,    0,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "mask\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "max_target_len\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 5\n",
    "batches = batch2TrainData(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "\n",
    "input_variable, lengths, output_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable\")\n",
    "print(input_variable)\n",
    "print(\"lengths\")\n",
    "print(lengths)\n",
    "print(\"output_varialbe\")\n",
    "print(output_variable)\n",
    "print(\"mask\")\n",
    "print(mask)\n",
    "print(\"max_target_len\")\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout), bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "    \n",
    "    def foward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        # tranpose\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "        \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.tranpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative log likelihood loss\n",
    "def maskNLLLoss(decoder_out, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    target = target.view(-1, 1)\n",
    "    # decoder_output shape (batch_size, vocab_size), target shape (batch_size, 1)\n",
    "    gathered_tensor = torch.gather(decoder_out, 1, target)\n",
    "    crossEntropy = -torch.log(gathered_tensor)\n",
    "    loss = crossEntropy.masked_select(mask)\n",
    "    loss = loss.mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2161, 0.1423, 0.0926, 0.1370, 0.2191, 0.0905, 0.1024],\n",
      "        [0.1205, 0.1489, 0.0760, 0.1321, 0.1385, 0.2028, 0.1812],\n",
      "        [0.1987, 0.1490, 0.0834, 0.1197, 0.0986, 0.1541, 0.1966],\n",
      "        [0.1532, 0.0952, 0.1828, 0.1512, 0.1300, 0.1554, 0.1323],\n",
      "        [0.2124, 0.1519, 0.1196, 0.1458, 0.1382, 0.1082, 0.1240]])\n",
      "tensor([[2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0]])\n",
      "tensor([[0.0926],\n",
      "        [0.1489],\n",
      "        [0.1541],\n",
      "        [0.1300],\n",
      "        [0.2124]])\n",
      "torch.Size([5, 1])\n",
      "Cross Entropy:\n",
      "tensor([[2.3790],\n",
      "        [1.9043],\n",
      "        [1.8699],\n",
      "        [2.0405],\n",
      "        [1.5494]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "tensor([2.3790, 1.8699, 2.0405])\n",
      "torch.Size([3])\n",
      "Sum of mask elements(How many elements we are considering):  tensor(3)\n",
      "Mean of loss:  tensor(2.0965)\n",
      "Mean of the cross-entropy loss (without masking):  tensor(1.9486)\n"
     ]
    }
   ],
   "source": [
    "# Visualize what's happening in the loss function\n",
    "dec_o = torch.rand(5, 7)\n",
    "dec_o = F.softmax(dec_o, dim=1)\n",
    "tar = torch.tensor([2, 1, 5, 4, 0], dtype=torch.long)\n",
    "tar = tar.view(-1, 1)\n",
    "mask = torch.tensor([1, 0, 1, 1, 0], dtype=torch.uint8)\n",
    "print(dec_o)\n",
    "print(tar)\n",
    "gath_ten = torch.gather(dec_o, 1, tar)\n",
    "print(gath_ten)\n",
    "print(gath_ten.shape)\n",
    "crossEntropy = -torch.log(gath_ten)\n",
    "print(\"Cross Entropy:\")\n",
    "print(crossEntropy)\n",
    "mask = mask.unsqueeze(1)\n",
    "loss = crossEntropy.masked_select(mask)\n",
    "print(\"Loss:\")\n",
    "print(loss)\n",
    "print(loss.shape)\n",
    "print(\"Sum of mask elements(How many elements we are considering): \", mask.sum())\n",
    "print(\"Mean of loss: \", loss.mean())\n",
    "print(\"Mean of the cross-entropy loss (without masking): \", crossEntropy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable shape:  torch.Size([9, 5])\n",
      "lengths shape:  torch.Size([5])\n",
      "target_variable:  torch.Size([6, 5])\n",
      "mask shape:  torch.Size([6, 5])\n",
      "max_target_len:  6\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 5\n",
    "batches = batch2TrainData(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable shape: \", input_variable.shape)\n",
    "print(\"lengths shape: \", lengths.shape)\n",
    "print(\"target_variable: \", target_variable.shape)\n",
    "print(\"mask shape: \", mask.shape)\n",
    "print(\"max_target_len: \", max_target_len)\n",
    "\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "attn_model = \"dot\"\n",
    "embedding = nn.Embedding(vocab.num_words, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(7826, 500)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(7826, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=7826, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variable = input_variable.to(device)\n",
    "lengths = lengths.to(device)\n",
    "target_variable = target_variable.to(device)\n",
    "mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "print_losses = []\n",
    "n_totals = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder outputs shape:  torch.Size([9, 5, 500])\n",
      "Last encoder hidden shape:  torch.Size([4, 5, 500])\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "print(\"Encoder outputs shape: \", encoder_outputs.shape)\n",
    "print(\"Last encoder hidden shape: \", encoder_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial decoder input shape:  torch.Size([1, 5])\n",
      "tensor([[1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "decoder_input = torch.LongTensor([[SOS_token for _ in range(small_batch_size)]])\n",
    "decoder_input = decoder_input.to(device)\n",
    "print(\"Initial decoder input shape: \", decoder_input.shape)\n",
    "print(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "print(\"Initial decoder hidden state layers: \", decoder_hidden.shape)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Now let's look what's happening in every timestep of GRU!\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for t in range(max_target_len):\n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    print(\"Decoder output shape: \", decoder_output.shape)\n",
    "    print(\"Decoder hidden shape: \", decoder_hidden.shape)\n",
    "    decoder_input = target_variable[t].view(-1, 1)\n",
    "    print(\"The target variable at the current timestep before reshaping: \", target_variable[t])\n",
    "    print(\"The target variable at the current timestep shape before reshaping: \", target_variable[t].shape)\n",
    "    print(\"The decoder input shape (reshape the target variable): \", decoder_input.shape)\n",
    "    print(\"The mask at the current timestep: \", mask[t])\n",
    "    print(\"The mask at the current timestep shape: \", mask[t].shape)\n",
    "    mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "    print(\"Mask Loss: \", mask_loss)\n",
    "    print(\"nTotal: \", nTotal)\n",
    "    loss += mask_loss\n",
    "    print_losses.append(mask_loss.item() * nTotal)\n",
    "    print(print_losses)\n",
    "    n_totals += nTotal\n",
    "    print(\"nTotals: \", n_totals)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    returned_loss = sum(print_losses) / n_totals\n",
    "    print(\"Returned Loss: \", returned_loss)\n",
    "    print(\"\\n\")\n",
    "    print(\"-------------DONE ONE TIME STEP--------------------\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
